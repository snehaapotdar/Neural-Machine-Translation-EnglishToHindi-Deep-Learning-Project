{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Copy of ADAM_30.ipynb","version":"0.3.2","provenance":[{"file_id":"1QK5vnmHWSedOn8kQYcL12Q8E-J37pf2s","timestamp":1564592523568}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YkIKl7ln7sD7","colab_type":"code","outputId":"cb45456f-c8e5-4a62-ca4e-cc115983d623","executionInfo":{"status":"ok","timestamp":1564587475293,"user_tz":420,"elapsed":2381,"user":{"displayName":"Lakshmi Madhuri Yalamanchi","photoUrl":"","userId":"08296936309529054876"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from __future__ import print_function\n","#import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","import numpy as np\n","import pandas as pd\n","import nltk\n","import matplotlib.pyplot as plt\n","pd.set_option('display.max_columns', None) "],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YD-BZGRQ7sD_","colab_type":"code","colab":{}},"source":["batch_size = 64  # Batch size for training.\n","epochs = 70  # Number of epochs to train for.\n","latent_dim = 512  # Latent dimensionality of the encoding space.\n","num_samples = 7000  # Number of samples to train on.\n","# Path to the data txt file on disk.\n","data_path = 'cleaned_data.txt'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4mcn16PS0ToV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"25c18454-c264-4a4b-aa59-1fc29c7247a8","executionInfo":{"status":"ok","timestamp":1564587475792,"user_tz":420,"elapsed":2824,"user":{"displayName":"Lakshmi Madhuri Yalamanchi","photoUrl":"","userId":"08296936309529054876"}}},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"RoL7Ciz-7sEB","colab_type":"code","outputId":"e05a18b8-9610-486e-e340-3a7200bbcab7","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1564587479887,"user_tz":420,"elapsed":6893,"user":{"displayName":"Lakshmi Madhuri Yalamanchi","photoUrl":"","userId":"08296936309529054876"}}},"source":["# Vectorize the data.\n","input_texts = []\n","target_texts = []\n","input_words = set()\n","target_words = set()\n","\n","with open(data_path, 'r', encoding='utf-8') as f:\n","    lines = f.read().split('\\n')\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    index, input_text, target_text = line.split('\\t')\n","    # We use \"tab\" as the \"start sequence\" character\n","    # for the targets, and \"\\n\" as \"end sequence\" character.\n","    target_text = 'START_ '+target_text+ ' _END'\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    \n","    input_word_tokens=nltk.word_tokenize(input_text)\n","    target_word_tokens=nltk.word_tokenize(target_text)\n","\n","    for word in input_word_tokens:\n","        if word not in input_words:\n","            input_words.add(word)\n","    for word in target_word_tokens:\n","        if word not in target_words:\n","            target_words.add(word)\n","#input_words.add('')\n","#target_words.add('')\n","input_words = sorted(list(input_words))\n","\n","target_words = sorted(list(target_words))\n","\n","num_encoder_tokens = len(input_words)\n","num_decoder_tokens = len(target_words)\n","max_encoder_seq_length = max([len(nltk.word_tokenize(txt)) for txt in input_texts])\n","max_decoder_seq_length = max([len(nltk.word_tokenize(txt)) for txt in target_texts])\n","\n","print('Number of samples:', len(input_texts))\n","print('Number of unique input tokens:', num_encoder_tokens)\n","print('Number of unique output tokens:', num_decoder_tokens)\n","print('Max sequence length for inputs:', max_encoder_seq_length)\n","\n","print('Max sequence length for outputs:', max_decoder_seq_length)\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Number of samples: 7000\n","Number of unique input tokens: 6570\n","Number of unique output tokens: 6478\n","Max sequence length for inputs: 43\n","Max sequence length for outputs: 43\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7l-IKfsL7sED","colab_type":"code","colab":{}},"source":["input_token_index = dict(\n","    [(word, i) for i, word in enumerate(input_words)])\n","target_token_index = dict(\n","    [(word, i) for i, word in enumerate(target_words)])\n","\n","encoder_input_data = np.zeros(\n","    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n","    dtype='float16')\n","decoder_input_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float16')\n","\n","decoder_target_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float16')\n","\n","for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, word in enumerate(nltk.word_tokenize(input_text)):\n","        encoder_input_data[i, t, input_token_index[word]] = 1.\n","\n","    for t, word in enumerate(nltk.word_tokenize(target_text)):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[word]] = 1.\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[word]] = 1."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kpG6_toQ7sEF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"5796e5fc-d94f-48fb-9f29-c1dab1c01616","executionInfo":{"status":"ok","timestamp":1564587490269,"user_tz":420,"elapsed":17222,"user":{"displayName":"Lakshmi Madhuri Yalamanchi","photoUrl":"","userId":"08296936309529054876"}}},"source":["#EARLY STOPPING\n","#early_stopping = EarlyStopping(monitor='val_loss', patience=25)\n","#MODEL CHECKPOINT\n","ckpt_file = 'model.28_jul_19'\n","checkpoint = ModelCheckpoint(ckpt_file, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","# Define an input sequence and process it.\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","# We set up our decoder to return full output sequences,\n","# and to return internal states as well. We don't use the\n","# return states in the training model, but we will use them in inference.\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n","                                     initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model that will turn\n","# `encoder_input_data` & `decoder_input_data` into `decoder_target_d"],"execution_count":10,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0731 15:38:08.794262 139851291961216 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0731 15:38:08.831473 139851291961216 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0731 15:38:08.839006 139851291961216 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"T1oZW6-87sEH","colab_type":"code","outputId":"2ed76a59-c68b-49af-aa80-9dcacafa7bf8","colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"status":"ok","timestamp":1564587490271,"user_tz":420,"elapsed":17201,"user":{"displayName":"Lakshmi Madhuri Yalamanchi","photoUrl":"","userId":"08296936309529054876"}}},"source":["model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.summary()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, None, 6570)   0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, None, 6478)   0                                            \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   [(None, 512), (None, 14505984    input_1[0][0]                    \n","__________________________________________________________________________________________________\n","lstm_2 (LSTM)                   [(None, None, 512),  14317568    input_2[0][0]                    \n","                                                                 lstm_1[0][1]                     \n","                                                                 lstm_1[0][2]                     \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, None, 6478)   3323214     lstm_2[0][0]                     \n","==================================================================================================\n","Total params: 32,146,766\n","Trainable params: 32,146,766\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hzi4ilDE7sEJ","colab_type":"code","outputId":"7766fe8f-c8bc-437f-e30c-03d22a07242a","colab":{"base_uri":"https://localhost:8080/","height":428},"executionInfo":{"status":"ok","timestamp":1564587490273,"user_tz":420,"elapsed":17181,"user":{"displayName":"Lakshmi Madhuri Yalamanchi","photoUrl":"","userId":"08296936309529054876"}}},"source":["model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n","\n","model.summary()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["W0731 15:38:10.185629 139851291961216 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0731 15:38:10.208654 139851291961216 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, None, 6570)   0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            (None, None, 6478)   0                                            \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   [(None, 512), (None, 14505984    input_1[0][0]                    \n","__________________________________________________________________________________________________\n","lstm_2 (LSTM)                   [(None, None, 512),  14317568    input_2[0][0]                    \n","                                                                 lstm_1[0][1]                     \n","                                                                 lstm_1[0][2]                     \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, None, 6478)   3323214     lstm_2[0][0]                     \n","==================================================================================================\n","Total params: 32,146,766\n","Trainable params: 32,146,766\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H6w0kcoC7sEK","colab_type":"code","outputId":"b27c187a-fa49-43ab-f5d9-9c33f6ebcffd","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1564591263562,"user_tz":420,"elapsed":99283,"user":{"displayName":"Lakshmi Madhuri Yalamanchi","photoUrl":"","userId":"08296936309529054876"}}},"source":["history=model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=batch_size,\n","          epochs=70,\n","          validation_split=0.2, callbacks=[checkpoint], verbose=1)\n","# Save model\n","model.save('Project_1.h5')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["W0731 15:38:10.526632 139851291961216 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0731 15:38:11.693939 139851291961216 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 5600 samples, validate on 1400 samples\n","Epoch 1/70\n","5600/5600 [==============================] - 59s 11ms/step - loss: 1.3042 - acc: 0.0241 - val_loss: 1.2082 - val_acc: 0.0270\n","\n","Epoch 00001: val_loss improved from inf to 1.20824, saving model to model.28_jul_19\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n","  '. They will not be included '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 2/70\n","5600/5600 [==============================] - 53s 9ms/step - loss: 1.1744 - acc: 0.0267 - val_loss: 1.2034 - val_acc: 0.0280\n","\n","Epoch 00002: val_loss improved from 1.20824 to 1.20341, saving model to model.28_jul_19\n","Epoch 3/70\n","5600/5600 [==============================] - 53s 9ms/step - loss: 1.1507 - acc: 0.0274 - val_loss: 1.1881 - val_acc: 0.0281\n","\n","Epoch 00003: val_loss improved from 1.20341 to 1.18813, saving model to model.28_jul_19\n","Epoch 4/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 1.1168 - acc: 0.0310 - val_loss: 1.1589 - val_acc: 0.0361\n","\n","Epoch 00004: val_loss improved from 1.18813 to 1.15887, saving model to model.28_jul_19\n","Epoch 5/70\n","5600/5600 [==============================] - 53s 9ms/step - loss: 1.0796 - acc: 0.0376 - val_loss: 1.1354 - val_acc: 0.0403\n","\n","Epoch 00005: val_loss improved from 1.15887 to 1.13540, saving model to model.28_jul_19\n","Epoch 6/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 1.0476 - acc: 0.0424 - val_loss: 1.1170 - val_acc: 0.0431\n","\n","Epoch 00006: val_loss improved from 1.13540 to 1.11703, saving model to model.28_jul_19\n","Epoch 7/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 1.0256 - acc: 0.0456 - val_loss: 1.1038 - val_acc: 0.0467\n","\n","Epoch 00007: val_loss improved from 1.11703 to 1.10383, saving model to model.28_jul_19\n","Epoch 8/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.9966 - acc: 0.0483 - val_loss: 1.0901 - val_acc: 0.0467\n","\n","Epoch 00008: val_loss improved from 1.10383 to 1.09015, saving model to model.28_jul_19\n","Epoch 9/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.9745 - acc: 0.0502 - val_loss: 1.0822 - val_acc: 0.0486\n","\n","Epoch 00009: val_loss improved from 1.09015 to 1.08224, saving model to model.28_jul_19\n","Epoch 10/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.9569 - acc: 0.0514 - val_loss: 1.0773 - val_acc: 0.0488\n","\n","Epoch 00010: val_loss improved from 1.08224 to 1.07732, saving model to model.28_jul_19\n","Epoch 11/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.9317 - acc: 0.0534 - val_loss: 1.0710 - val_acc: 0.0502\n","\n","Epoch 00011: val_loss improved from 1.07732 to 1.07097, saving model to model.28_jul_19\n","Epoch 12/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.9128 - acc: 0.0553 - val_loss: 1.0704 - val_acc: 0.0508\n","\n","Epoch 00012: val_loss improved from 1.07097 to 1.07042, saving model to model.28_jul_19\n","Epoch 13/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.8994 - acc: 0.0565 - val_loss: 1.0685 - val_acc: 0.0515\n","\n","Epoch 00013: val_loss improved from 1.07042 to 1.06851, saving model to model.28_jul_19\n","Epoch 14/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.8815 - acc: 0.0578 - val_loss: 1.0659 - val_acc: 0.0525\n","\n","Epoch 00014: val_loss improved from 1.06851 to 1.06591, saving model to model.28_jul_19\n","Epoch 15/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.8548 - acc: 0.0600 - val_loss: 1.0637 - val_acc: 0.0520\n","\n","Epoch 00015: val_loss improved from 1.06591 to 1.06375, saving model to model.28_jul_19\n","Epoch 16/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.8338 - acc: 0.0617 - val_loss: 1.0643 - val_acc: 0.0540\n","\n","Epoch 00016: val_loss did not improve from 1.06375\n","Epoch 17/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.8158 - acc: 0.0638 - val_loss: 1.0685 - val_acc: 0.0538\n","\n","Epoch 00017: val_loss did not improve from 1.06375\n","Epoch 18/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.7961 - acc: 0.0659 - val_loss: 1.0662 - val_acc: 0.0559\n","\n","Epoch 00018: val_loss did not improve from 1.06375\n","Epoch 19/70\n","5600/5600 [==============================] - 53s 9ms/step - loss: 0.7755 - acc: 0.0683 - val_loss: 1.0688 - val_acc: 0.0556\n","\n","Epoch 00019: val_loss did not improve from 1.06375\n","Epoch 20/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.7534 - acc: 0.0711 - val_loss: 1.0710 - val_acc: 0.0569\n","\n","Epoch 00020: val_loss did not improve from 1.06375\n","Epoch 21/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.7339 - acc: 0.0733 - val_loss: 1.0708 - val_acc: 0.0579\n","\n","Epoch 00021: val_loss did not improve from 1.06375\n","Epoch 22/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.7148 - acc: 0.0755 - val_loss: 1.0751 - val_acc: 0.0582\n","\n","Epoch 00022: val_loss did not improve from 1.06375\n","Epoch 23/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.6935 - acc: 0.0781 - val_loss: 1.0833 - val_acc: 0.0593\n","\n","Epoch 00023: val_loss did not improve from 1.06375\n","Epoch 24/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.6746 - acc: 0.0812 - val_loss: 1.0874 - val_acc: 0.0589\n","\n","Epoch 00024: val_loss did not improve from 1.06375\n","Epoch 25/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.6549 - acc: 0.0837 - val_loss: 1.0897 - val_acc: 0.0580\n","\n","Epoch 00025: val_loss did not improve from 1.06375\n","Epoch 26/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.6336 - acc: 0.0866 - val_loss: 1.0954 - val_acc: 0.0600\n","\n","Epoch 00026: val_loss did not improve from 1.06375\n","Epoch 27/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.6138 - acc: 0.0900 - val_loss: 1.1012 - val_acc: 0.0600\n","\n","Epoch 00027: val_loss did not improve from 1.06375\n","Epoch 28/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.5937 - acc: 0.0934 - val_loss: 1.1091 - val_acc: 0.0634\n","\n","Epoch 00028: val_loss did not improve from 1.06375\n","Epoch 29/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.5749 - acc: 0.0977 - val_loss: 1.1156 - val_acc: 0.0599\n","\n","Epoch 00029: val_loss did not improve from 1.06375\n","Epoch 30/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.5532 - acc: 0.1018 - val_loss: 1.1243 - val_acc: 0.0634\n","\n","Epoch 00030: val_loss did not improve from 1.06375\n","Epoch 31/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.5347 - acc: 0.1048 - val_loss: 1.1328 - val_acc: 0.0599\n","\n","Epoch 00031: val_loss did not improve from 1.06375\n","Epoch 32/70\n","5600/5600 [==============================] - 53s 10ms/step - loss: 0.5146 - acc: 0.1077 - val_loss: 1.1409 - val_acc: 0.0599\n","\n","Epoch 00032: val_loss did not improve from 1.06375\n","Epoch 33/70\n","5600/5600 [==============================] - 53s 9ms/step - loss: 0.4950 - acc: 0.1121 - val_loss: 1.1584 - val_acc: 0.0639\n","\n","Epoch 00033: val_loss did not improve from 1.06375\n","Epoch 34/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.4788 - acc: 0.1150 - val_loss: 1.1576 - val_acc: 0.0600\n","\n","Epoch 00034: val_loss did not improve from 1.06375\n","Epoch 35/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.4605 - acc: 0.1180 - val_loss: 1.1679 - val_acc: 0.0626\n","\n","Epoch 00035: val_loss did not improve from 1.06375\n","Epoch 36/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.4388 - acc: 0.1232 - val_loss: 1.1724 - val_acc: 0.0580\n","\n","Epoch 00036: val_loss did not improve from 1.06375\n","Epoch 37/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.4210 - acc: 0.1254 - val_loss: 1.1831 - val_acc: 0.0616\n","\n","Epoch 00037: val_loss did not improve from 1.06375\n","Epoch 38/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.4045 - acc: 0.1285 - val_loss: 1.1945 - val_acc: 0.0605\n","\n","Epoch 00038: val_loss did not improve from 1.06375\n","Epoch 39/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.3893 - acc: 0.1312 - val_loss: 1.1987 - val_acc: 0.0596\n","\n","Epoch 00039: val_loss did not improve from 1.06375\n","Epoch 40/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.3722 - acc: 0.1354 - val_loss: 1.2131 - val_acc: 0.0586\n","\n","Epoch 00040: val_loss did not improve from 1.06375\n","Epoch 41/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.3560 - acc: 0.1382 - val_loss: 1.2244 - val_acc: 0.0605\n","\n","Epoch 00041: val_loss did not improve from 1.06375\n","Epoch 42/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.3405 - acc: 0.1407 - val_loss: 1.2323 - val_acc: 0.0598\n","\n","Epoch 00042: val_loss did not improve from 1.06375\n","Epoch 43/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.3266 - acc: 0.1436 - val_loss: 1.2472 - val_acc: 0.0561\n","\n","Epoch 00043: val_loss did not improve from 1.06375\n","Epoch 44/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.3130 - acc: 0.1458 - val_loss: 1.2552 - val_acc: 0.0578\n","\n","Epoch 00044: val_loss did not improve from 1.06375\n","Epoch 45/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.2975 - acc: 0.1492 - val_loss: 1.2624 - val_acc: 0.0591\n","\n","Epoch 00045: val_loss did not improve from 1.06375\n","Epoch 46/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.2852 - acc: 0.1508 - val_loss: 1.2703 - val_acc: 0.0579\n","\n","Epoch 00046: val_loss did not improve from 1.06375\n","Epoch 47/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.2725 - acc: 0.1539 - val_loss: 1.2803 - val_acc: 0.0570\n","\n","Epoch 00047: val_loss did not improve from 1.06375\n","Epoch 48/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.2620 - acc: 0.1553 - val_loss: 1.2923 - val_acc: 0.0565\n","\n","Epoch 00048: val_loss did not improve from 1.06375\n","Epoch 49/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.2503 - acc: 0.1579 - val_loss: 1.3019 - val_acc: 0.0577\n","\n","Epoch 00049: val_loss did not improve from 1.06375\n","Epoch 50/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.2386 - acc: 0.1605 - val_loss: 1.3143 - val_acc: 0.0550\n","\n","Epoch 00050: val_loss did not improve from 1.06375\n","Epoch 51/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.2273 - acc: 0.1623 - val_loss: 1.3165 - val_acc: 0.0572\n","\n","Epoch 00051: val_loss did not improve from 1.06375\n","Epoch 52/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.2154 - acc: 0.1653 - val_loss: 1.3269 - val_acc: 0.0546\n","\n","Epoch 00052: val_loss did not improve from 1.06375\n","Epoch 53/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.2061 - acc: 0.1669 - val_loss: 1.3398 - val_acc: 0.0551\n","\n","Epoch 00053: val_loss did not improve from 1.06375\n","Epoch 54/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.1971 - acc: 0.1690 - val_loss: 1.3504 - val_acc: 0.0550\n","\n","Epoch 00054: val_loss did not improve from 1.06375\n","Epoch 55/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.1866 - acc: 0.1713 - val_loss: 1.3571 - val_acc: 0.0576\n","\n","Epoch 00055: val_loss did not improve from 1.06375\n","Epoch 56/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.1800 - acc: 0.1723 - val_loss: 1.3730 - val_acc: 0.0540\n","\n","Epoch 00056: val_loss did not improve from 1.06375\n","Epoch 57/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.1707 - acc: 0.1748 - val_loss: 1.3750 - val_acc: 0.0571\n","\n","Epoch 00057: val_loss did not improve from 1.06375\n","Epoch 58/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.1601 - acc: 0.1774 - val_loss: 1.3963 - val_acc: 0.0533\n","\n","Epoch 00058: val_loss did not improve from 1.06375\n","Epoch 59/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.1541 - acc: 0.1790 - val_loss: 1.3938 - val_acc: 0.0560\n","\n","Epoch 00059: val_loss did not improve from 1.06375\n","Epoch 60/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.1451 - acc: 0.1807 - val_loss: 1.4043 - val_acc: 0.0560\n","\n","Epoch 00060: val_loss did not improve from 1.06375\n","Epoch 61/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.1376 - acc: 0.1828 - val_loss: 1.4124 - val_acc: 0.0540\n","\n","Epoch 00061: val_loss did not improve from 1.06375\n","Epoch 62/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.1304 - acc: 0.1841 - val_loss: 1.4244 - val_acc: 0.0546\n","\n","Epoch 00062: val_loss did not improve from 1.06375\n","Epoch 63/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.1215 - acc: 0.1865 - val_loss: 1.4336 - val_acc: 0.0532\n","\n","Epoch 00063: val_loss did not improve from 1.06375\n","Epoch 64/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.1151 - acc: 0.1880 - val_loss: 1.4530 - val_acc: 0.0517\n","\n","Epoch 00064: val_loss did not improve from 1.06375\n","Epoch 65/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.1083 - acc: 0.1895 - val_loss: 1.4595 - val_acc: 0.0540\n","\n","Epoch 00065: val_loss did not improve from 1.06375\n","Epoch 66/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.1030 - acc: 0.1913 - val_loss: 1.4630 - val_acc: 0.0549\n","\n","Epoch 00066: val_loss did not improve from 1.06375\n","Epoch 67/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.0981 - acc: 0.1921 - val_loss: 1.4712 - val_acc: 0.0534\n","\n","Epoch 00067: val_loss did not improve from 1.06375\n","Epoch 68/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.0910 - acc: 0.1932 - val_loss: 1.4799 - val_acc: 0.0517\n","\n","Epoch 00068: val_loss did not improve from 1.06375\n","Epoch 69/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.0882 - acc: 0.1947 - val_loss: 1.4882 - val_acc: 0.0546\n","\n","Epoch 00069: val_loss did not improve from 1.06375\n","Epoch 70/70\n","5600/5600 [==============================] - 54s 10ms/step - loss: 0.0829 - acc: 0.1961 - val_loss: 1.4994 - val_acc: 0.0534\n","\n","Epoch 00070: val_loss did not improve from 1.06375\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BY6gcPI67sEM","colab_type":"code","colab":{}},"source":["encoder_model = Model(encoder_inputs, encoder_states)\n","\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZaTT48eg7sEP","colab_type":"code","colab":{}},"source":["# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_word_index = dict(\n","    (i, word) for word, i in input_token_index.items())\n","reverse_target_word_index = dict(\n","    (i, word) for word, i in target_token_index.items())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9zCjFs027sES","colab_type":"code","colab":{}},"source":["def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index['START_']] = 1.\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while stop_condition == False:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_word = reverse_target_word_index[sampled_token_index]\n","        decoded_sentence += ' '+sampled_word\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if (sampled_word is '_END' or len(decoded_sentence) > max_decoder_seq_length):\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e-YalfSs7sEU","colab_type":"code","outputId":"18f33517-4339-43f7-ef61-6a1a9ecdc46f","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1564591426580,"user_tz":420,"elapsed":5632,"user":{"displayName":"Lakshmi Madhuri Yalamanchi","photoUrl":"","userId":"08296936309529054876"}}},"source":["from nltk.translate.bleu_score import sentence_bleu\n","for seq_index in range(100):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    target_sentence = target_texts[seq_index]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Target sentence:', target_sentence)\n","    print('Decoded sentence:', decoded_sentence)\n","    \n","    score = nltk.translate.bleu_score.sentence_bleu([target_sentence],decoded_sentence,weights =[1])\n","    print ('Bleuscore',score)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["-\n","Input sentence: I do not want to die.\n","Target sentence: START_ मैं मरना नहीं चाहता. _END\n","Decoded sentence:  मैं मरना नहीं चाहता . _END लगता है . _END केवल\n","Bleuscore 0.574468085106383\n","-\n","Input sentence: It's the same country I think.\n","Target sentence: START_ यह मुझे लगता है कि एक ही देश है. _END\n","Decoded sentence:  यह मुझे लगता है कि एक ही देश है . _END कोई रास्ता\n","Bleuscore 0.76\n","-\n","Input sentence: Then they'll be crying like babies.\n","Target sentence: START_ फिर ये नन्हें बच्चों की तरह रोएँगे। _END\n","Decoded sentence:  जो यहाँ से भी है . भारत के लिए मुझे फोन करता\n","Bleuscore 0.5526610492595279\n","-\n","Input sentence: - No, I need power up!\n","Target sentence: START_ नहीं, मुझे पावर की जरुरत है ! _END\n","Decoded sentence:  नहीं , मुझे पावर की जरुरत है ! _END _END _END\n","Bleuscore 0.782608695652174\n","-\n","Input sentence: I will not eat him.\n","Target sentence: START_ मैं उसे नहीं खा जाएगा. _END\n","Decoded sentence:  मैं उसे नहीं खा जाएगा . _END ठीक है . _END सिर्फ\n","Bleuscore 0.5918367346938775\n","-\n","Input sentence: You gotta get me to Charleston.\n","Target sentence: START_ आप चार्ल्सटन करने के लिए मुझे जाना होगा. _END\n","Decoded sentence:  आप चार्ल्सटन करने के लिए मुझे जाना होगा . _END\n","Bleuscore 0.8799508864282417\n","-\n","Input sentence: - NO, HE'S NOT MY DAD.\n","Target sentence: START_ - नहीं, वह मेरे पिता नहीं है. _END\n","Decoded sentence:  - नहीं , वह मेरे पिता नहीं है . _END कहा . _END\n","Bleuscore 0.75\n","-\n","Input sentence: I told her we rest on Sundays.\n","Target sentence: START_ मैं रविवार को उसे हम बाकी बताया. _END\n","Decoded sentence:  मैं रविवार को उसे हम बाकी बताया . _END सुरक्षित\n","Bleuscore 0.7916666666666666\n","-\n","Input sentence: You could've at least informed me, right?\n","Target sentence: START_ तुम्हें कम से कम मुझे तो बताना चाहिए था,ना? _END\n","Decoded sentence:  तुम्हें यकीन है ? _END तो ? _END वहाँ भी न ?\n","Bleuscore 0.5338249352778719\n","-\n","Input sentence: Your little bitch says you're gonna put me in jail!\n","Target sentence: START_ तेरी कमीनी कहती है कि वो मुझे जेल भेजेगी ! _END\n","Decoded sentence:  तेरी कमीनी कहती है कि वो मुझे जेल भेजेगी ! _END\n","Bleuscore 0.8824969025845955\n","-\n","Input sentence: - You can call me whatever you like.\n","Target sentence: START_ - तुम मुझे फोन कर सकते हैं जो कुछ भी आप की तरह। _END\n","Decoded sentence:  - तुम मुझे फोन कर सकते हैं जो कुछ भी हम जीना\n","Bleuscore 0.6675095817179522\n","-\n","Input sentence: - You don't just kill a guy like that!\n","Target sentence: START_ - तुम बस की तरह है कि एक आदमी को मार नहीं है! _END\n","Decoded sentence:  - तुम चुप हूँ ? _END वहाँ कुछ है , एक ना ? _END\n","Bleuscore 0.5526860787869335\n","-\n","Input sentence: You sent these?\n","Target sentence: START_ आप इन भेजा? _END\n","Decoded sentence:  आप इन भेजा ? _END सुरक्षित कहा कि वो सब कुछ\n","Bleuscore 0.38636363636363635\n","-\n","Input sentence: I really loved him.\n","Target sentence: START_ मैं वास्तव में उसे प्यार करता था। _END\n","Decoded sentence:  मैं वास्तव में उसे प्यार करता था। _END सोच रहा\n","Bleuscore 0.8297872340425532\n","-\n","Input sentence: I ain't much at guessing games.\n","Target sentence: START_ मैं अनुमान लगाने के खेल में ज्यादा नहीं है. _END\n","Decoded sentence:  मैं अनुमान लगाने के खेल में ज्यादा नहीं है .\n","Bleuscore 0.800737402916808\n","-\n","Input sentence: You're sick and I can help you.\n","Target sentence: START_ तुम बीमार हो और मैं तुम्हारी मदद कर सकते हैं। _END\n","Decoded sentence:  तुम बीमार हो और मैं तुम्हारी मदद कर सकते हैं।\n","Bleuscore 0.7873121827816278\n","-\n","Input sentence: Mike, do I get to ride with you?\n","Target sentence: START_ माइक, मैं आप के साथ सवारी करने के लिए मिलता है? _END\n","Decoded sentence:  माइक , मैं आप के साथ सवारी करने के लिए मिलता\n","Bleuscore 0.7326324677392158\n","-\n","Input sentence: What do you fucking think?\n","Target sentence: START_ आपको क्या लगता है कि बकवास है? _END\n","Decoded sentence:  तुम मुझे क्या करना चाहते हैं ? _END ? _END मोहब्बत\n","Bleuscore 0.5294117647058824\n","-\n","Input sentence: I know that woman you love also is ready to forgive you.\n","Target sentence: START_ मैं आप उसे माफ करने के लिए तैयार भी प्यार औरत को जानते हैं. _END\n","Decoded sentence:  मैं तुम्हें यह कहना है कि तुम मुझे पता है .\n","Bleuscore 0.418338036742475\n","-\n","Input sentence: Don't do it, man.\n","Target sentence: START_ , आदमी ऐसा मत करो. _END\n","Decoded sentence:  , यह नहीं है . _END लगता है। _END केवल केवल\n","Bleuscore 0.38636363636363635\n","-\n","Input sentence: - Say sorry right now.\n","Target sentence: START_ अब ठीक है माफी माँगने। _END\n","Decoded sentence:  अब ठीक है माफी माँगने। _END . _END . _END _END\n","Bleuscore 0.6170212765957447\n","-\n","Input sentence: It'll be okay.\n","Target sentence: START_ यह ठीक हो जाएगा. _END\n","Decoded sentence:  यह ठीक हो जाएगा . _END ! _END _END _END _END\n","Bleuscore 0.5111111111111111\n","-\n","Input sentence: - It's not personal.\n","Target sentence: START_ -यह कोई निजी दुश्मनी नहीं। _END\n","Decoded sentence:  -यह कोई निजी दुश्मनी नहीं। _END _END _END _END\n","Bleuscore 0.7021276595744681\n","-\n","Input sentence: Will I have enough time to do it? The poison takes effect after 1 or 2 seconds.\n","Target sentence: START_ जहर का प्रभाव होता है 1 या 2 सेकंड के बाद. _END\n","Decoded sentence:  जहर का प्रभाव होता है 1 या 2 सेकंड के बाद .\n","Bleuscore 0.7967034698934615\n","-\n","Input sentence: - Did you file the football yet?\n","Target sentence: START_ - अगर आप अभी तक फुटबॉल दायर की थी? _END\n","Decoded sentence:  - अगर आप अभी तक फुटबॉल दायर की थी ? _END _END\n","Bleuscore 0.8913043478260869\n","-\n","Input sentence: You cannot be serious.\n","Target sentence: START_ आप गंभीर नहीं किया जा सकता है. _END\n","Decoded sentence:  आप गंभीर नहीं किया जा सकता है . _END _END _END\n","Bleuscore 0.7872340425531915\n","-\n","Input sentence: Turn over in my mind is okay, but allow me to find out your not.\n","Target sentence: START_ मेरे दिमाग में मुड़ें पर ठीक है, लेकिन अनुमति नहीं खोजने के लिए मुझे बाहर अपने. _END\n","Decoded sentence:  मेरे पति अब एक वर्ष बाहर है . _END कहा कि उसने\n","Bleuscore 0.34206799435677193\n","-\n","Input sentence: Feels like there's something in there.\n","Target sentence: START_ वहाँ में कुछ है जैसे लगता है. _END\n","Decoded sentence:  वहाँ में कुछ है जैसे लगता है . _END केवल एक\n","Bleuscore 0.7954545454545454\n","-\n","Input sentence: This is your honeymoon suite.\n","Target sentence: START_ यह अपने हनीमून सुइट है। _END\n","Decoded sentence:  यह अपने हनीमून सुइट है। _END _END ... _END तुम्हारा\n","Bleuscore 0.5769230769230769\n","-\n","Input sentence: and religious interests to agree on a single treaty..\n","Target sentence: START_ बाईस अरब देश एक ही संधि पर तैयार हो जाएंगे _END\n","Decoded sentence:  बाईस अरब देश एक ही संधि पर तैयार हो जाएंगे _END\n","Bleuscore 0.8824969025845955\n","-\n","Input sentence: Let's be real hot (nicer) I am of you\n","Target sentence: START_ चलो तुम हो गरम रियल (अच्छे) का रहा हूँ मैं _END\n","Decoded sentence:  चलो तुम हो गरम रियल ( अच्छे ) का रहा हूँ मैं\n","Bleuscore 0.8005367363429157\n","-\n","Input sentence: Look, I know I quit the academy before.\n","Target sentence: START_ मैं मैं पहले अकादमी छोड़ने पता है, देखो. _END\n","Decoded sentence:  मैं मैं इसे अकादमी छोड़ने पता है , देखो , मैं\n","Bleuscore 0.7250678666244116\n","-\n","Input sentence: We village elders still exist.\n","Target sentence: START_ हम गांव के बुजुर्ग अभी भी मौजूद हैं. _END\n","Decoded sentence:  हम गांव के बुजुर्ग अभी भी मौजूद हैं . _END सिर्फ\n","Bleuscore 0.8571428571428571\n","-\n","Input sentence: It's like a 1 man reign of terror.\n","Target sentence: START_ यह आतंक का एक आदमी 1 शासनकाल की तरह है। _END\n","Decoded sentence:  यह आतंक का एक आदमी 1 शासनकाल की तरह है। _END\n","Bleuscore 0.8751733190429475\n","-\n","Input sentence: - I need to secure the genesis chamber and pay my respects to an old friend.\n","Target sentence: START_ - हाँ साहब _END\n","Decoded sentence:  - हाँ साहब _END धन्यवाद . _END _END . _END इससे\n","Bleuscore 0.3541666666666667\n","-\n","Input sentence: I was your ghost.\n","Target sentence: START_ मैं था तुम्हारा भूत. _END\n","Decoded sentence:  मैं हूँ ... ... _END की तलाशी _END _END केवल\n","Bleuscore 0.4\n","-\n","Input sentence: - You don't know what the honor is.\n","Target sentence: START_ - आप नहीं जानते कि सम्मान क्या है _END\n","Decoded sentence:  - आप नहीं जानते कि सम्मान क्या है _END _END\n","Bleuscore 0.8886627693209018\n","-\n","Input sentence: This is my office.\n","Target sentence: START_ यह मेरा कार्यालय है. _END\n","Decoded sentence:  यह मेरा कार्यालय है . _END _END . _END केवल\n","Bleuscore 0.6136363636363636\n","-\n","Input sentence: Okkoto's done for. Leave him!\n","Target sentence: START_ वह के लिए किया जाता है. _END\n","Decoded sentence:  यह नहीं लगता है कि देर हो चुकी है जो वह है .\n","Bleuscore 0.4666666666666667\n","-\n","Input sentence: This is a fucking showroom!\n","Target sentence: START_ - यह साला एक शोरूम है! _END\n","Decoded sentence:  - यह साला एक शोरूम है ! _END कहा . _END इससे\n","Bleuscore 0.6444444444444445\n","-\n","Input sentence: Move back now!\n","Target sentence: START_ -फ़ौरन पीछे हटो ! _END\n","Decoded sentence:  -फ़ौरन पीछे हटो ! _END ! _END विमान , लेकिन\n","Bleuscore 0.5454545454545454\n","-\n","Input sentence: You are so much better than that.\n","Target sentence: START_ तुम इतना है कि तुलना में बेहतर हैं. _END\n","Decoded sentence:  आप कुछ नहीं देख सकते हैं लेकिन मुझे यहां से\n","Bleuscore 0.5731919734978325\n","-\n","Input sentence: I'll go to the watchtower and check with Ji-san.\n","Target sentence: START_ तुम वापस घर जाओ. _END\n","Decoded sentence:  तुम वापस घर जाओ . _END केवल अभी भी एक , बेटे\n","Bleuscore 0.4888888888888889\n","-\n","Input sentence: We need another exit.\n","Target sentence: START_ हमें कोई और निकास चाहिए। _END\n","Decoded sentence:  हमें कोई और निकास चाहिए। _END मैं यहाँ आया के\n","Bleuscore 0.6521739130434783\n","-\n","Input sentence: - I sure can\n","Target sentence: START_ - मुझसे बिलकुल हो जाएगा _END\n","Decoded sentence:  - मेरे लिये ? _END वहाँ से कहा है _END _END\n","Bleuscore 0.4772727272727273\n","-\n","Input sentence: I'm surprised they've let it go on this long.\n","Target sentence: START_ मेरे लिए आश्चर्य के रूप में समय की अनुमति निम्नानुसार है. शायद वे इसके पीछे हैं. _END\n","Decoded sentence:  मेरे लिए आश्चर्य के रूप में समय की एक पैकेट\n","Bleuscore 0.3130079597910377\n","-\n","Input sentence: What are you really up to?\n","Target sentence: START_ क्या तुम सच में करने के लिए क्या कर रहे हैं? _END\n","Decoded sentence:  क्या कोई भी एक बंदूक थी ? _END कोई भी मतलब है\n","Bleuscore 0.559732214487202\n","-\n","Input sentence: - WOMAN. ;\n","Target sentence: START_ - महिला: _END\n","Decoded sentence:  - महिला : _END 3 _END कभी नहीं नहीं है। _END\n","Bleuscore 0.3333333333333333\n","-\n","Input sentence: This is how it works.\n","Target sentence: START_ यह कैसे काम करता है. भारत _END\n","Decoded sentence:  यह बहुत अच्छा था है . _END _END . _END केवल\n","Bleuscore 0.5\n","-\n","Input sentence: Please move aside.\n","Target sentence: START_ चल सामने से हट. _END\n","Decoded sentence:  कृपया भगवान के लिए बॉबी घंटे _END पुत्र। _END\n","Bleuscore 0.3478260869565218\n","-\n","Input sentence: It'll wait.\n","Target sentence: START_ यह इंतजार करेंगे. _END\n","Decoded sentence:  यह इंतजार करेंगे . _END _END होगा। _END _END\n","Bleuscore 0.5333333333333333\n","-\n","Input sentence: Don't look astonished; she is an accomplished thief\n","Target sentence: START_ आशचर्य से मत देखो, वह पूरी चोर है _END\n","Decoded sentence:  आशचर्य से मत देखो , वह पूरी चोर है _END हैं\n","Bleuscore 0.8664462000878793\n","-\n","Input sentence: Will you?\n","Target sentence: START_ वचन नहीं निभाओगे? _END\n","Decoded sentence:  वचन नहीं निभाओगे ? _END ? _END ? _END _END अभी\n","Bleuscore 0.5106382978723404\n","-\n","Input sentence: Let's just get the money, see what happens.\n","Target sentence: START_ चलो बस पैसे मिल देखते हैं, क्या होता है. _END\n","Decoded sentence:  . _END कोई भी नहीं तो न ही - _END तुम्हें अब\n","Bleuscore 0.5516054706434599\n","-\n","Input sentence: Everybody, come on!\n","Target sentence: START_ , वापस जाओ! , _END\n","Decoded sentence:  , वापस आओ , , _END ! _END केवल मुझे क्या होता\n","Bleuscore 0.41304347826086957\n","-\n","Input sentence: For the lady!\n","Target sentence: START_ महिला के लिए! _END\n","Decoded sentence:  चलो ... _END ! _END जल्दी जल्दी ही दो ! ♪ _END\n","Bleuscore 0.276595744680851\n","-\n","Input sentence: Keep coming.\n","Target sentence: START_ आते रहना. _END\n","Decoded sentence:  कृपया दूर ख्याल रखना . धन्यवाद . _END _END केवल\n","Bleuscore 0.2708333333333333\n","-\n","Input sentence: I will find you!\n","Target sentence: START_ मैं तुम्हें मिल जाएगा! _END\n","Decoded sentence:  मैं तुम्हें मिल गया था _END _END कहा जाता है\n","Bleuscore 0.6\n","-\n","Input sentence: Our agency passed suspension order against him\n","Target sentence: START_ हमारे एजेंसी उनके खिलाफ निलंबन आदेश पारित _END\n","Decoded sentence:  हमारे एजेंसी उनके खिलाफ निलंबन आदेश पारित _END\n","Bleuscore 0.880152954687935\n","-\n","Input sentence: I need.. I need to call my wife.\n","Target sentence: START_ - मुझे अपनी पत्नी को फ़ोन करना है। _END\n","Decoded sentence:  - मुझे अपनी पत्नी को फ़ोन करना है। _END _END\n","Bleuscore 0.891087506041525\n","-\n","Input sentence: Do I eat other's brains?\n","Target sentence: START_ मैं किसी का दिमाग खाता हूँ? _END\n","Decoded sentence:  मैं हैरान हूँ . _END , ठीक है , मुझे आश्चर्य\n","Bleuscore 0.4666666666666667\n","-\n","Input sentence: WELL, IT'S A BIT ODD NOT TO.\n","Target sentence: START_ वैसे, ऐसा नहीं करने के लिए एक अजीब सा है. _END\n","Decoded sentence:  वैसे , ऐसा नहीं करने के लिए एक अजीब सा है .\n","Bleuscore 0.7964945653890284\n","-\n","Input sentence: Whatever mistakes I made, I've paid for them and then some.\n","Target sentence: START_ जो गलतियां मैंने की, उनका भुगतान कर चुका हूं. _END\n","Decoded sentence:  जो गलतियां मैंने की , उनका भुगतान कर चुका हूं\n","Bleuscore 0.7873121827816278\n","-\n","Input sentence: He asked me for a length of rope.\n","Target sentence: START_ - उसने मुझ से एक रस्सी मांगी. _END\n","Decoded sentence:  - उसने मुझ से एक रस्सी मांगी . _END कहा था उसने\n","Bleuscore 0.7291666666666666\n","-\n","Input sentence: Go ahead!\n","Target sentence: START_ आगे बढ़ो! _END\n","Decoded sentence:  आगे बढ़ो ! _END ! _END धन्यवाद , लेकिन मैं अब\n","Bleuscore 0.3478260869565218\n","-\n","Input sentence: Notice anything strange about your boss?\n","Target sentence: START_ अपने मालिक के बारे में नोटिस कुछ अजीब ? _END\n","Decoded sentence:  अपने पिता की मृत्यु का खयाल , आप इसके साथ भाग\n","Bleuscore 0.5070019087315528\n","-\n","Input sentence: Of course I feel sad\n","Target sentence: START_ बेशक मुझे दुख होता है _END\n","Decoded sentence:  बेशक मुझे दुख होता है _END केवल ' ' है , जहां\n","Bleuscore 0.5869565217391305\n","-\n","Input sentence: I did not get anything.\n","Target sentence: START_ मैं कुछ भी नहीं मिला! _END\n","Decoded sentence:  मैं कुछ भी नहीं हूँ ! _END केवल की तरह लग रहा\n","Bleuscore 0.5434782608695652\n","-\n","Input sentence: Thank you for the help.\n","Target sentence: START_ आप की मदद के लिए धन्यवाद. _END\n","Decoded sentence:  मेरे बच्चे .. _END वहाँ से बढ़ ~ ~ ' ? _END\n","Bleuscore 0.4090909090909091\n","-\n","Input sentence: - Look at that.\n","Target sentence: START_ - उसे देखो. हे, भगवान! _END\n","Decoded sentence:  - मुझे देखो . _END है ) _END कहा जाता है . _END\n","Bleuscore 0.4166666666666667\n","-\n","Input sentence: In times like this, security is more important than liberty.\n","Target sentence: START_ सुरक्षा और अधिक स्वतंत्रता से अधिक महत्वपूर्ण है! _END\n","Decoded sentence:  सुरक्षा और अधिक स्वतंत्रता से अधिक महत्वपूर्ण\n","Bleuscore 0.7217424244921066\n","-\n","Input sentence: I have a friend named Vito.\n","Target sentence: START_ मैं वीटो नाम का एक दोस्त है. _END\n","Decoded sentence:  मैं वीटो नाम का एक दोस्त है . _END कहा जाता\n","Bleuscore 0.7727272727272727\n","-\n","Input sentence: Never a doubt.\n","Target sentence: START_ कभी एक संदेह. _END\n","Decoded sentence:  यहाँ पर जाओ ! _END लेकिन ठीक है। _END नीचे आ\n","Bleuscore 0.31111111111111106\n","-\n","Input sentence: Can I go up there?\n","Target sentence: START_ मैं वहाँ ऊपर जांऊ? _END\n","Decoded sentence:  मैं वहाँ ऊपर जांऊ ? _END क्या कहा ? _END केवल\n","Bleuscore 0.5434782608695652\n","-\n","Input sentence: you could be the bridge between 2 peoples.\n","Target sentence: START_ आप पुल हो सकता है 2 लोगों के बीच. _END\n","Decoded sentence:  आप पुल हो सकता है 2 लोगों के बीच . _END अपने\n","Bleuscore 0.8666666666666667\n","-\n","Input sentence: Come, Macedonians.\n","Target sentence: START_ मकिदुनियों, आओ. _END\n","Decoded sentence:  मकिदुनियों , आओ . _END धन्यवाद . _END हैं .\n","Bleuscore 0.5\n","-\n","Input sentence: How's her hair?\n","Target sentence: START_ उसके बाल कैसे हैं? _END\n","Decoded sentence:  उसके बाल कैसे हैं ? _END ? _END तो जहां तक फ़ोन\n","Bleuscore 0.5319148936170213\n","-\n","Input sentence: Hey, hey.\n","Target sentence: START_ ऐ। _END\n","Decoded sentence:  हेमहे . _END _END _END _END . _END कहते हैं\n","Bleuscore 0.1590909090909091\n","-\n","Input sentence: Whoa! Astrid!\n","Target sentence: START_ ऐस्ट्रिड! _END\n","Decoded sentence:  ऐस्ट्रिड ! _END सर . _END अंत . _END कहते हैं\n","Bleuscore 0.3478260869565218\n","-\n","Input sentence: Let's not repeat the same mistakes that we made in the past.\n","Target sentence: START_ हमें पहले की गई ग़लतियां दोहरानी नहीं चाहिए। _END\n","Decoded sentence:  हमें पहले की गई ग़लतियां दोहरानी नहीं चाहिए।\n","Bleuscore 0.7831394949065555\n","-\n","Input sentence: Jack always told me that if anything should ever happened to him.\n","Target sentence: START_ जैक ने मुझे हमेशा बताया कि अगर उसके साथ कुछ हुआ... _END\n","Decoded sentence:  जैक ने मुझे हमेशा बताया कि हम उसके साथ कुछ हुआ\n","Bleuscore 0.6958410312515492\n","-\n","Input sentence: I want you to come home.\n","Target sentence: START_ जब तक आप घर नहीं मिलते _END\n","Decoded sentence:  मैंने सोचा अब तुम से उसे बताने नहीं करती था\n","Bleuscore 0.4090909090909091\n","-\n","Input sentence: Carver just told me we're not gonna tell anybody what happened up there.\n","Target sentence: START_ कार्वर ने बताया कि हमें उस घटना का ज़िक्र किसी से नहीं करना है। _END\n","Decoded sentence:  कार्वर ने बताया कि हमें उस घटना का ज़िक्र किसी\n","Bleuscore 0.5511519806833008\n","-\n","Input sentence: Prove to us you are who you say you are.\n","Target sentence: START_ हमें सिद्ध करो ... ... तुम जो कहते हो, वो हो. _END\n","Decoded sentence:  हमें सिद्ध करो ... ... आप की कहते हैं कि लेकिन\n","Bleuscore 0.5847604314349271\n","-\n","Input sentence: The Spirit Realm.\n","Target sentence: START_ आत्मा मंडल। _END\n","Decoded sentence:  आत्मा मंडल। _END ताला में से ही क्यों _END जाते\n","Bleuscore 0.375\n","-\n","Input sentence: As for Superman, he was in the room, but obviously failed to stop him.\n","Target sentence: START_ सुपरमैन के रूप में, वह कमरे में था, लेकिन स्पष्ट रूप से उसे रोकने में नाकाम रहे। _END\n","Decoded sentence:  बस के लिए उसके पास आता है ? _END कहा कि कि एक\n","Bleuscore 0.2959030287683341\n","-\n","Input sentence: Thank you, Alex.\n","Target sentence: START_ धन्यवाद, एलेक्स! _END\n","Decoded sentence:  धन्यवाद , एलेक्स ! _END उच्च ' पर त्वचा के वरिष्ठ\n","Bleuscore 0.44\n","-\n","Input sentence: Transportation is waiting for you.\n","Target sentence: START_ परिवहन आप के लिए इंतज़ार कर रहा है। _END\n","Decoded sentence:  परिवहन आप के लिए इंतज़ार कर रहा है। _END जंगल\n","Bleuscore 0.872137259113984\n","-\n","Input sentence: How much do I get out of it?\n","Target sentence: START_ मैं इसे से बाहर निकलना है? _END\n","Decoded sentence:  मैं यहाँ से बाहर निकलना है ? _END केवल आज है\n","Bleuscore 0.6666666666666666\n","-\n","Input sentence: Your bloody pills are making me feel like shit.\n","Target sentence: START_ तुम्हारी दवाइयां मुझे बीमार बना रही हैं. _END\n","Decoded sentence:  तुम्हारी दवाइयां मुझे बीमार बना रही हैं . _END\n","Bleuscore 0.8799508864282417\n","-\n","Input sentence: Hey, they want me to attend this meeting so much they're gonna fly me all the way to New York.\n","Target sentence: START_ अरे, वे मुझे इस बैठक में भाग लेने के लिए इतना वे ये मुझे न्यूयॉर्क के लिए सभी तरह से उड़ रहे हैं चाहता हूँ. _END\n","Decoded sentence:  अरे , तुम मुझे सुनने के लिए नहीं जा रहा है !\n","Bleuscore 0.18024516700093848\n","-\n","Input sentence: - Big shots!\n","Target sentence: START_ - बिग शॉट! _END\n","Decoded sentence:  - बिग शॉट ! _END : कृपया जल्दी से वे तुम्हें\n","Bleuscore 0.35555555555555557\n","-\n","Input sentence: Why is this right for you and wrong for me?\n","Target sentence: START_ क्यों आप के लिए यह सही है और मेरे लिए गलत क्या है? _END\n","Decoded sentence:  क्यों तुम्हें पता है कि तुम लोग ? _END जानते\n","Bleuscore 0.5178447754456351\n","-\n","Input sentence: - I know what I'm doing. - Uh-oh.\n","Target sentence: START_ - मैं मैं क्या कर रहा हूँ. _END\n","Decoded sentence:  - मैं मैं क्या कर रहा हूँ . _END कहा . _END\n","Bleuscore 0.75\n","-\n","Input sentence: Hang on tight!\n","Target sentence: START_ तंग पर पकड़ो! _END\n","Decoded sentence:  बॉब ! _END ! _END ठीक हो जाओ ! _END सिर्फ हमारी\n","Bleuscore 0.2708333333333333\n","-\n","Input sentence: On a quest for the truth of who I am.\n","Target sentence: START_ मैं कबूल करता हूं कि मेरी खोज छोड़ने से पहले मैं मर जाऊँगा _END\n","Decoded sentence:  मैं कबूल करता हूं कि मेरी खोज छोड़ने से पहले\n","Bleuscore 0.5737534207374327\n","-\n","Input sentence: I spoke to Walker.\n","Target sentence: START_ मैंने वॉकर से बात की थी। _END\n","Decoded sentence:  मैंने वॉकर से बात की थी। _END _END _END _END\n","Bleuscore 0.6888888888888889\n","-\n","Input sentence: I mean, we really need a phone.\n","Target sentence: START_ मेरा मतलब है, फोन निहायत जरूरी है. _END\n","Decoded sentence:  मेरा मतलब है , फोन निहायत जरूरी है . _END केवल\n","Bleuscore 0.851063829787234\n","-\n","Input sentence: Brice, sometimes we can get comfortable with people.\n","Target sentence: START_ ब्राईस, कभी कभी हम लोगों के साथ सहज प्राप्त कर सकते हैं. _END\n","Decoded sentence:  ब्राईस , कभी कभी हम लोगों के साथ सहज प्राप्त\n","Bleuscore 0.5998287320415084\n","-\n","Input sentence: Including Mr Baptiste.\n","Target sentence: START_ श्री बैप्टिस्ट भी शामिल है. _END\n","Decoded sentence:  श्री बैप्टिस्ट भी शामिल है . _END केवल एक बड़ी\n","Bleuscore 0.717391304347826\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H1gyotNI7sEX","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}